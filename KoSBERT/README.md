# Ko-Sentence-BERT
ğŸŒ· Korean SentenceBERT : Sentence Embeddings using Siamese BERT-Networks using ETRI KoBERT and kakaobrain KorNLU dataset

## Installation
 - ETRI KorBERTëŠ” transformers 2.4.1 ~ 2.8.0ì—ì„œë§Œ ë™ì‘í•˜ê³  Sentence-BERTëŠ” 3.1.0 ë²„ì „ ì´ìƒì—ì„œ ë™ì‘í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤. <br>
 - **huggingface transformer, sentence transformers, tokenizers** ë¼ì´ë¸ŒëŸ¬ë¦¬ ì½”ë“œë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ë¯€ë¡œ ê°€ìƒí™˜ê²½ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.  <br>
 - ì‚¬ìš©í•œ Docker imageëŠ” Docker Hubì— ì²¨ë¶€í•©ë‹ˆë‹¤. <br>
     - https://hub.docker.com/r/klbm126/kosbert_image/tags <br>
 - ETRI KoBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ê³  ë³¸ ë ˆíŒŒì§€í† ë¦¬ì—ì„  ETRI KoBERTë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. <br>
 - **SKT KoBERT**ë¥¼ ì‚¬ìš©í•œ ë²„ì „ì€ ë‹¤ìŒ ë ˆíŒŒì§€í† ë¦¬ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. <br>
     - https://github.com/BM-K/KoSentenceBERT_SKTBERT <br>
```
git clone https://github.com/BM-K/KoSentenceBERT.git
python -m venv .KoSBERT
. .KoSBERT/bin/activate
pip install -r requirements.txt
```
 - transformer, tokenizers, sentence_transformers ë””ë ‰í† ë¦¬ë¥¼ .KoSBERT/lib/python3.7/site-packages/ ë¡œ ì´ë™í•©ë‹ˆë‹¤. <br>
 - ETRI_KoBERT ëª¨ë¸ê³¼ tokenizerê°€ KoSentenceBERT ë””ë ‰í† ë¦¬ ì•ˆì— ì¡´ì¬í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.<br>
 - ETRI ëª¨ë¸ê³¼ tokenizerëŠ” ë‹¤ìŒ ì˜ˆì‹œì™€ ê°™ì´ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ :

 ```python
 from ETRI_tok.tokenization_etri_eojeol import BertTokenizer
 self.auto_model = BertModel.from_pretrained('./ETRI_KoBERT/003_bert_eojeol_pytorch') 
 self.tokenizer = BertTokenizer.from_pretrained('./ETRI_KoBERT/003_bert_eojeol_pytorch/vocab.txt', do_lower_case=False)
 ```

## Train Models
 - ëª¨ë¸ í•™ìŠµì„ ì›í•˜ì‹œë©´ KoSentenceBERT ë””ë ‰í† ë¦¬ ì•ˆì— KorNLUDatasetsì´ ì¡´ì¬í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤. <br>
 - STS í•™ìŠµ ì‹œ ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ë°ì´í„°ì™€ í•™ìŠµ ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤ : <br><br>
KoSentenceBERT/KorNLUDatasets/KorSTS/tune_test.tsv <br>
<img src="https://user-images.githubusercontent.com/55969260/93304207-97afec00-f837-11ea-88a2-7256f2f1664e.png"></img><br>
*STS test ë°ì´í„°ì…‹ì˜ ì¼ë¶€* <br>
```
python training_nli.py      # NLI ë°ì´í„°ë¡œë§Œ í•™ìŠµ
python training_sts.py      # STS ë°ì´í„°ë¡œë§Œ í•™ìŠµ
python con_training_sts.py  # NLI ë°ì´í„°ë¡œ í•™ìŠµ í›„ STS ë°ì´í„°ë¡œ Fine-Tuning
```

## Pre-Trained Models
**pooling mode**ëŠ” **MEAN-strategy**ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, í•™ìŠµì‹œ ëª¨ë¸ì€ output ë””ë ‰í† ë¦¬ì— ì €ì¥ ë©ë‹ˆë‹¤. <br>
|ë””ë ‰í† ë¦¬|í•™ìŠµë°©ë²•|
|-----------|:----:|
|training_**nli**_ETRI_KoBERT-003_bert_eojeol|Only Train NLI|
|training_**sts**_ETRI_KoBERT-003_bert_eojeol|Only Train STS|
|training_**nli_sts**_ETRI_KoBERT-003_bert_eojeol|STS + NLI|

## Performance
Seed ê³ ì •, test set
|Model|Cosine Pearson|Cosine Spearman|Euclidean Pearson|Euclidean Spearman|Manhattan Pearson|Manhattan Spearman|Dot Pearson|Dot Spearman|
|:------------------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|NLl|67.96|70.45|71.06|70.48|71.17|70.51|64.87|63.04|
|STS|**80.43**|79.99|78.18|78.03|78.13|77.99|73.73|73.40|
|STS + NLI|80.10|**80.42**|**79.14**|**79.28**|**79.08**|**79.22**|**74.46**|**74.16**|

## Application Examples
- ìƒì„± ëœ ë¬¸ì¥ ì„ë² ë”©ì„ ë‹¤ìš´ ìŠ¤íŠ¸ë¦¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•œ ëª‡ ê°€ì§€ ì˜ˆë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- STS + NLI pretrained ëª¨ë¸ì„ í†µí•´ ì§„í–‰í•©ë‹ˆë‹¤.

### Semantic Search
SemanticSearch.pyëŠ” ì£¼ì–´ì§„ ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì°¾ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.<br>
ë¨¼ì € Corpusì˜ ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

model_path = './output/training_nli_sts_ETRI_KoBERT-003_bert_eojeol'

embedder = SentenceTransformer(model_path)

# Corpus with example sentences
corpus = ['í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤.',
          'í•œ ë‚¨ìê°€ ë¹µ í•œ ì¡°ê°ì„ ë¨¹ëŠ”ë‹¤.',
          'ê·¸ ì—¬ìê°€ ì•„ì´ë¥¼ ëŒë³¸ë‹¤.',
          'í•œ ë‚¨ìê°€ ë§ì„ íƒ„ë‹¤.',
          'í•œ ì—¬ìê°€ ë°”ì´ì˜¬ë¦°ì„ ì—°ì£¼í•œë‹¤.',
          'ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤.',
          'í•œ ë‚¨ìê°€ ë‹´ìœ¼ë¡œ ì‹¸ì¸ ë•…ì—ì„œ ë°±ë§ˆë¥¼ íƒ€ê³  ìˆë‹¤.',
          'ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤.',
          'ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤.']

corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = ['í•œ ë‚¨ìê°€ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ëŠ”ë‹¤.',
           'ê³ ë¦´ë¼ ì˜ìƒì„ ì…ì€ ëˆ„êµ°ê°€ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•˜ê³  ìˆë‹¤.',
           'ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.']

# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = 5
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
    cos_scores = cos_scores.cpu()

    #We use np.argpartition, to only partially sort the top_k results
    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for idx in top_results[0:top_k]:
        print(corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]))
        
```
<br> ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :
```
========================


Query: í•œ ë‚¨ìê°€ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ëŠ”ë‹¤.

Top 5 most similar sentences in corpus:
í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤. (Score: 0.7557)
í•œ ë‚¨ìê°€ ë¹µ í•œ ì¡°ê°ì„ ë¨¹ëŠ”ë‹¤. (Score: 0.6464)
í•œ ë‚¨ìê°€ ë‹´ìœ¼ë¡œ ì‹¸ì¸ ë•…ì—ì„œ ë°±ë§ˆë¥¼ íƒ€ê³  ìˆë‹¤. (Score: 0.2565)
í•œ ë‚¨ìê°€ ë§ì„ íƒ„ë‹¤. (Score: 0.2333)
ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤. (Score: 0.1792)


========================


Query: ê³ ë¦´ë¼ ì˜ìƒì„ ì…ì€ ëˆ„êµ°ê°€ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•˜ê³  ìˆë‹¤.

Top 5 most similar sentences in corpus:
ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤. (Score: 0.6732)
ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤. (Score: 0.3401)
ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤. (Score: 0.1037)
í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤. (Score: 0.0617)
ê·¸ ì—¬ìê°€ ì•„ì´ë¥¼ ëŒë³¸ë‹¤. (Score: 0.0466)


=======================


Query: ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.

Top 5 most similar sentences in corpus:
ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤. (Score: 0.7164)
ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤. (Score: 0.3216)
ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤. (Score: 0.2071)
í•œ ë‚¨ìê°€ ë¹µ í•œ ì¡°ê°ì„ ë¨¹ëŠ”ë‹¤. (Score: 0.1089)
í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤. (Score: 0.0724)
```
### Clustering
Clustering.pyëŠ” ë¬¸ì¥ ì„ë² ë”© ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” ì˜ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. <br>
ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë¨¼ì € ê° ë¬¸ì¥ì— ëŒ€í•œ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤. <br>
```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

model_path = './output/training_nli_sts_ETRI_KoBERT-003_bert_eojeol'

embedder = SentenceTransformer(model_path)

# Corpus with example sentences
corpus = ['í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤.',
          'í•œ ë‚¨ìê°€ ë¹µ í•œ ì¡°ê°ì„ ë¨¹ëŠ”ë‹¤.',
          'ê·¸ ì—¬ìê°€ ì•„ì´ë¥¼ ëŒë³¸ë‹¤.',
          'í•œ ë‚¨ìê°€ ë§ì„ íƒ„ë‹¤.',
          'í•œ ì—¬ìê°€ ë°”ì´ì˜¬ë¦°ì„ ì—°ì£¼í•œë‹¤.',
          'ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤.',
          'í•œ ë‚¨ìê°€ ë‹´ìœ¼ë¡œ ì‹¸ì¸ ë•…ì—ì„œ ë°±ë§ˆë¥¼ íƒ€ê³  ìˆë‹¤.',
          'ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤.',
          'ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤.',
          'í•œ ë‚¨ìê°€ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ëŠ”ë‹¤.',
          'ê³ ë¦´ë¼ ì˜ìƒì„ ì…ì€ ëˆ„êµ°ê°€ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•˜ê³  ìˆë‹¤.',
          'ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.']

corpus_embeddings = embedder.encode(corpus)

# Then, we perform k-means clustering using sklearn:
from sklearn.cluster import KMeans

num_clusters = 5
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print("Cluster ", i+1)
    print(cluster)
    print("")

```
ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :
 ```
 Cluster  1
['ë‘ ë‚¨ìê°€ ìˆ˜ë ˆë¥¼ ìˆ² ì†ìœ¼ë¡œ ë°€ì—ˆë‹¤.', 'ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤.', 'ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.']

Cluster  2
['í•œ ë‚¨ìê°€ ë§ì„ íƒ„ë‹¤.', 'í•œ ë‚¨ìê°€ ë‹´ìœ¼ë¡œ ì‹¸ì¸ ë•…ì—ì„œ ë°±ë§ˆë¥¼ íƒ€ê³  ìˆë‹¤.']

Cluster  3
['í•œ ë‚¨ìê°€ ìŒì‹ì„ ë¨¹ëŠ”ë‹¤.', 'í•œ ë‚¨ìê°€ ë¹µ í•œ ì¡°ê°ì„ ë¨¹ëŠ”ë‹¤.', 'í•œ ë‚¨ìê°€ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ëŠ”ë‹¤.']

Cluster  4
['ê·¸ ì—¬ìê°€ ì•„ì´ë¥¼ ëŒë³¸ë‹¤.', 'í•œ ì—¬ìê°€ ë°”ì´ì˜¬ë¦°ì„ ì—°ì£¼í•œë‹¤.']

Cluster  5
['ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤.', 'ê³ ë¦´ë¼ ì˜ìƒì„ ì…ì€ ëˆ„êµ°ê°€ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•˜ê³  ìˆë‹¤.']
```
<img src = "https://user-images.githubusercontent.com/55969260/96250228-78001500-0fe9-11eb-9ee5-914705182a55.png">

## Downstream Tasks Demo
<img src = "https://user-images.githubusercontent.com/55969260/99897723-73610780-2cdf-11eb-9b71-3d31a309a53a.gif"> <br>
<img src = "https://user-images.githubusercontent.com/55969260/99897743-94295d00-2cdf-11eb-9b16-d6fec66e43d0.gif"> <br>
<img src = "https://user-images.githubusercontent.com/55969260/99897764-a73c2d00-2cdf-11eb-8bcf-0235d1fda0f7.gif"> <br>

## Citing
### KorNLU Datasets
```bibtex
@article{ham2020kornli,
  title={KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding},
  author={Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},
  journal={arXiv preprint arXiv:2004.03289},
  year={2020}
}
```
### Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa & Co. with PyTorch
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}

@article{reimers-2020-multilingual-sentence-bert,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils and Gurevych, Iryna",
    journal= "arXiv preprint arXiv:2004.09813",
    month = "04",
    year = "2020",
    url = "http://arxiv.org/abs/2004.09813",
}
```

